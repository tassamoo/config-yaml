1. Amazon Simple Storage Service (Amazon S3) provides a good solution for which of the following use cases?
A. A data warehouse for business intelligence
B. An internet-accessible storage location for video files that an external website accesses - TRUE
C. Hourly storage of frequently accessed temporary files
D. A cluster for traditional Apache Spark and Apache Hadoop installations to process big data

Web developers can use Amazon S3 to store large video files without provisioning storage. External websites can access these files by using cross-origin resource sharing (CORS).

The best answer is **B. An internet-accessible storage location for video files that an external website accesses**. Amazon S3 is designed for large-capacity, low-cost file storage in one specific geographical region. The service provides developers with secure, durable, highly-scalable object storage. This makes it ideal for uses cases like storing and retrieving any amount of data, at any time, from anywhere on the web, such as video files for an external website.




2. A company is interested in using Amazon Simple Storage Service (Amazon S3) alone to host their website, instead of a traditional web server. Which types of content does Amazon S3 support for static web hosting? (Select THREE.)
1. HTML files and image files
2. Client-side scripts
3. Server-side scripts
4. Dynamic HTML files
5. Video and sound files

The best answers are:
1. **HTML files and image files**
2. **Client-side scripts**
5. **Video and sound files**

HTML files and image files are static content that can be
hosted by using Amazon S3 static web hosting. Client-
side scripts, video, and sound files create dynamic
presentations, but they are static content because their
files do not change on the web server.

Amazon S3 supports static web hosting and can serve HTML files, client-side scripts (like JavaScript), and multimedia content like video and sound files. However, it does not support server-side scripts or dynamic HTML files as it is not a traditional web server. It can only serve static content. For dynamic content, other AWS services like EC2 or Lambda can be used.

3. Which scenarios represent a good use for Amazon Simple Storage Service (Amazon S3)? (Select TWO.)
1. Housing the root volume of a live operating system
2. Providing a mountable file system for Linux-based workloads
3. Backing up critical data
4. Exposing a virtual tape library to on-premises backup systems
5. Storing computation and analytics data

The best answers are:
3. **Backing up critical data**
5. **Storing computation and analytics data**

Amazon S3 is a good choice to back up critical data, for cloud-based and on-premises systems. Amazon S3 can also store computation and large-scale analytics data, such as for financial transaction analysis, clickstream analytics, and media transcoding.

Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is ideal for backing up and restoring data for disaster recovery and storing computation and analytics data. However, it is not suitable for housing the root volume of a live operating system or providing a mountable file system for Linux-based workloads. For these use cases, other AWS services like Amazon EC2 and Amazon EFS would be more appropriate. Similarly, exposing a virtual tape library to on-premises backup systems would be better served by AWS Storage Gateway.

4. Here is the question in the correct format:

A company wants to use an S3 bucket to store sensitive data. Which actions can they take to protect their data? (Select TWO.)
1. Uploading unencrypted files to Amazon S3 because Amazon S3 encrypts the files by default
2. Enabling server-side encryption on the S3 bucket before uploading sensitive data
3. Enabling server-side encryption on the S3 bucket after uploading sensitive data
4. Using client-side encryption to protect data in transit
5. Using Secure File Transfer Protocol (SFTP) to connect directly to Amazon S3

The best answers are:
2. **Enabling server-side encryption on the S3 bucket before uploading sensitive data**
4. **Using client-side encryption to protect data in transit**

Amazon S3 server-side encryption must be enabled on a bucket before uploading objects. Existing objects must be re-uploaded to be encrypted at rest. Server-side encryption does not encrypt data in transit; use client- side encryption instead.

Amazon S3 provides a number of encryption options to protect data at rest and in transit. Enabling server-side encryption on the S3 bucket before uploading sensitive data ensures that Amazon S3 automatically encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. Using client-side encryption allows you to encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. However, Amazon S3 does not encrypt files by default, and SFTP cannot be used to connect directly to Amazon S3.

5. Here is the question in the correct format:

A company must create a common place to store shared files. Which requirements does Amazon Simple Storage Service (Amazon S3) support? (Select TWO.)
1. Recover deleted files
2. Maintain different versions of files
3. Lock a file so that only one person at a time can edit it
4. Attach comments to files
5. Compare file contents between files

The best answers are:
1. **Recover deleted files**
2. **Maintain different versions of files**

Amazon S3 object versioning enables maintenance of different versions and recovery of deleted files. It is not enabled by default, and it must be enabled on a bucket.

Amazon S3 supports both of these features. You can recover deleted files if you enable versioning on your S3 bucket. This allows you to retrieve all versions of an object (including all writes and deletes). Also, with S3 Versioning, you can preserve, retrieve, and restore every version of every object in your bucket. This makes it easier to recover from both unintended user actions and application failures. However, Amazon S3 does not natively support file locking, attaching comments to files, or comparing file contents between files.

6. A customer service team accesses case data daily for up to 30 days. Cases can be reopened and require immediate access for 1 year after they are closed. Which solution meets the requirements and is the most cost-efficient?
1. Store all case data in S3 Standard so that it is available whenever needed.
2. Store case data in S3 Standard. Use a lifecycle policy to move the data into S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
3. Store case data in S3 Standard. Use a lifecycle policy to move the data into Amazon S3 Glacier after 30 days.
4. Store case data in S3 Intelligent-Tiering to automatically move data between tiers based on access frequency.

The best answer is **2. Store case data in S3 Standard. Use a lifecycle policy to move the data into S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days**. 

S3 Standard-IA provides a lower storage cost for
infrequently accessed data, while still making the
data immediately available when needed.

Amazon S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics. 

After 30 days, moving the data to S3 Standard-IA is cost-efficient for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. 

Storing all data in S3 Standard (option 1) would be more expensive due to higher storage costs. Storing data in S3 Glacier (option 3) would not meet the requirement for immediate access. S3 Intelligent-Tiering (option 4) could be cost-efficient, but it incurs additional fees for monitoring and automation that may make it less cost-efficient than S3 Standard-IA for this use case.

7. Which Amazon Simple Storage Service (Amazon S3) unaccelerated data transfers have an associated cost? (Select TWO.)

1. IN from the internet
2. OUT to the internet
3. OUT to other AWS Regions
4. OUT to other AWS services in the same AWS Region
5. OUT to Amazon CloudFront

The best answers are:
2. **OUT to the internet**
3. **OUT to other AWS Regions**

Data transfer OUT to the internet has tiered pricing
per gigabyte (GB). Data transfer OUT to other AWS
services has a cost per GB. Check the Amazon S3
pricing documentation for current prices.

Data transfer **OUT to the internet** and **OUT to other AWS Regions** from Amazon S3 have associated costs. These costs are based on the amount of data transferred. However, data transfer IN from the internet to Amazon S3 is generally free. Also, data transfer OUT to Amazon CloudFront is free because CloudFront is part of Amazon's content delivery network, and data transfer between Amazon S3 and other AWS services in the same region is typically free as well.

8.A company is migrating 100 terabytes (TB) of data from their on-premises data center to Amazon Simple Storage Service (Amazon S3). The company connects to Amazon Web Services (AWS) by using a single 155 megabits per second (Mbps) internet connection. Which data transfer option is the fastest and most cost-effective?
1. AWS Management Console
2. Amazon S3 multipart uploads
3. AWS Snowball
4. AWS Snowmobile

The best answer is **3. AWS Snowball**.

AWS Snowball is good for transferring dozens of TB to
petabytes (PB) of data to AWS. Local data transfer to a
Snowball device is faster than transferring directly to
Amazon S3 over the internet. Transfer time to AWS
depends on physical package shipping times.

AWS Snowball is a petabyte-scale data transport solution that uses secure devices to transfer large amounts of data into and out of the AWS Cloud. With Snowball, you don’t need to write any code or purchase any hardware to transfer your data. Simply create a job in the AWS Management Console and a Snowball device will be automatically shipped to your location. 

For 100 TB of data and a 155 Mbps connection, transferring data over the internet would take a significant amount of time and could be costly. AWS Snowball is designed for large-scale migrations and can be faster and more cost-effective. AWS Management Console and Amazon S3 multipart uploads are not designed for such large-scale data transfers. AWS Snowmobile is used for even larger datasets, in the exabyte range, and would be an overkill for this scenario.

9. A video producer must regularly transfer several video files to Amazon Simple Storage Service (Amazon S3). The files range from 100—700 MB. The internet connection has been unreliable, causing some uploads to fail. Which solution provides the fastest, most reliable, and most cost-effective way to transfer these files to Amazon S3?

1. AWS Management Console
2. AWS Snowball
3. AWS Snowmobile
4. Amazon S3 multipart uploads

The best answer is **4. Amazon S3 multipart uploads**.

When using multipart uploads, an upload failure requires a restart of only failed part uploads instead of restarting the entire file upload. This method provides a better solution for unreliable connections.

Amazon S3 multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. This ability to retry or resume upload of individual parts can make uploading large objects more reliable and faster. 

AWS Management Console is not designed for handling large-scale data transfers or unreliable connections. AWS Snowball and AWS Snowmobile are used for large-scale data migrations (in the range of petabytes or exabytes), and would be an overkill for this scenario.

10. Which qualities vary by AWS Region? (Select TWO.)
1. Cost-effectiveness of workloads
2. Data privacy
3. High availability of workloads
4. Service and feature availability
5. Capacity for more customers

The best answers are:
1. **Cost-effectiveness of workloads**
4. **Service and feature availability**

The cost-effectiveness of workloads can vary by AWS Region due to differences in pricing for AWS services across regions. Similarly, service and feature availability can also vary as not all AWS services or features are available in every region. However, data privacy policies are consistent across all AWS regions, and high availability and capacity for more customers are designed to be robust across all regions.

Each AWS Region has its own pricing schedule, and service and feature availabilities vary by Region. Check to make sure that the Region that is selected for an architecture has the required services and features.

Prompt:
Fix this question in the right format (don't change any word) then answer the questions choose the best answer only: 
